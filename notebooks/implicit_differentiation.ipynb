{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "protecting-marine",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/google/jax-md/blob/main/notebooks/implicit_differentiation.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "official-indianapolis",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Import & Util\n",
    "!pip install -q git+https://www.github.com/google/jax\n",
    "!pip install -q git+https://www.github.com/google/jax-md\n",
    "!pip install jaxopt\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax.config import config\n",
    "config.update('jax_enable_x64', True)\n",
    "\n",
    "from jax import random, jit, lax\n",
    "\n",
    "from jax_md import space, energy, minimize, quantity\n",
    "\n",
    "import jaxopt\n",
    "\n",
    "f32 = jnp.float32\n",
    "f64 = jnp.float64"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "going-motion",
   "metadata": {},
   "source": [
    "# Energy minimization functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "judicial-union",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Energy minimization functions with and without neighbor lists.\n",
    "def run_minimization_while(\n",
    "    energy_fn, R_init, shift, max_grad_thresh=1e-12, max_num_steps=1000000, **kwargs\n",
    "):\n",
    "    init, apply = minimize.fire_descent(jit(energy_fn), shift, **kwargs)\n",
    "    apply = jit(apply)\n",
    "\n",
    "    @jit\n",
    "    def get_maxgrad(state):\n",
    "        return jnp.amax(jnp.abs(state.force))\n",
    "\n",
    "    @jit\n",
    "    def cond_fn(val):\n",
    "        state, i = val\n",
    "        return jnp.logical_and(get_maxgrad(state) > max_grad_thresh, i < max_num_steps)\n",
    "\n",
    "    @jit\n",
    "    def body_fn(val):\n",
    "        state, i = val\n",
    "        return apply(state), i + 1\n",
    "\n",
    "    state = init(R_init)\n",
    "    state, num_iterations = lax.while_loop(cond_fn, body_fn, (state, 0))\n",
    "\n",
    "    return state.position, get_maxgrad(state), num_iterations+1\n",
    "\n",
    "\n",
    "def run_minimization_while_nl(\n",
    "    neighbor_fn,\n",
    "    energy_fn,\n",
    "    R_init,\n",
    "    forced_rebuilding,\n",
    "    shift,\n",
    "    max_grad_thresh=1e-12,\n",
    "    max_num_steps=1000000,\n",
    "    **kwargs\n",
    "):\n",
    "    init_fn, apply_fn = minimize.fire_descent(energy_fn, shift, **kwargs)\n",
    "    apply_fn = jit(apply_fn)\n",
    "\n",
    "    nbrs = neighbor_fn.allocate(R_init)\n",
    "    state = init_fn(R_init, neighbor=nbrs)\n",
    "\n",
    "    @jit\n",
    "    def get_maxgrad(state):\n",
    "        return jnp.amax(jnp.abs(state.force))\n",
    "\n",
    "    @jit\n",
    "    def cond_fn(state, i):\n",
    "        return jnp.logical_and(get_maxgrad(state) > max_grad_thresh, i < max_num_steps)\n",
    "\n",
    "    @jit\n",
    "    def update_nbrs(R,nbrs):\n",
    "        return neighbor_fn.update(R,nbrs)\n",
    "    \n",
    "    steps = 0\n",
    "    while cond_fn(state, steps):\n",
    "        nbrs = update_nbrs(state.position,nbrs)\n",
    "        new_state = apply_fn(state, neighbor=nbrs)\n",
    "        if forced_rebuilding and steps == 10:\n",
    "            print(\"Forced rebuilding of neighbor_list.\")\n",
    "            nbrs = neighbor_fn.allocate(state.position)\n",
    "        if nbrs.did_buffer_overflow:\n",
    "            print(\"Rebuilding neighbor_list.\")\n",
    "            nbrs = neighbor_fn.allocate(state.position)\n",
    "        else:\n",
    "            state = new_state\n",
    "            steps += 1\n",
    "        \n",
    "    return state.position, nbrs, steps+1\n",
    "\n",
    "# This version is fully differentiable and jit-able\n",
    "def run_minimization_scan(force_fn, R_init, shift, num_steps=5000, **kwargs):\n",
    "    init, apply = minimize.fire_descent(jit(force_fn), shift, **kwargs)\n",
    "    apply = jit(apply)\n",
    "\n",
    "    @jit\n",
    "    def scan_fn(state, i):\n",
    "        return apply(state), 0.0\n",
    "\n",
    "    state = init(R_init)\n",
    "    state, _ = lax.scan(scan_fn, state, jnp.arange(num_steps))\n",
    "    return state.position, jnp.amax(jnp.abs(force_fn(state.position)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "introductory-priority",
   "metadata": {},
   "source": [
    "# Meta Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "industrial-tonight",
   "metadata": {},
   "source": [
    "In this notebook we'll have another look at differentiating through an energy minimization routine. But this time we'll look at how a technique called implicit differentiation lets us do so much more efficiently than before. Let us first set up our system and see for ourselves what goes wrong when we aren't using implicit differentiation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "liable-dodge",
   "metadata": {},
   "source": [
    "We'll work with a system of `N` soft spheres and we are interested in first computing the energy minimum of this system and then to compute the gradient of the energy with respect to the parameters `sigma` and/or `alpha`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wanted-consequence",
   "metadata": {},
   "source": [
    "We'll start of with a very small and rather dilute system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "creative-assault",
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 7\n",
    "density = 0.3\n",
    "\n",
    "dimension = 2\n",
    "box_size = quantity.box_size_at_number_density(N,density,dimension)\n",
    "displacement, shift = space.periodic(box_size) \n",
    "\n",
    "key = random.PRNGKey(5)\n",
    "key, split = random.split(key)\n",
    "R_init = random.uniform(key, (N,dimension), minval=0.0, maxval=box_size, dtype=f64) \n",
    "\n",
    "sigma = jnp.full((N,N), 2.)\n",
    "alpha = jnp.full((N,N), 2.)\n",
    "param_dict = {'sigma':sigma,'alpha':alpha}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wooden-drain",
   "metadata": {},
   "source": [
    "Note that we are using a scan for the `explicit` function in order to use `jax.grad`. Here the step size is fixed and we need to choose it large enough in order to reach the energy minimum!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "applicable-therapy",
   "metadata": {},
   "outputs": [],
   "source": [
    "def explicit_diff(params,R_init,displacement,num_steps):\n",
    "    energy_fn = energy.soft_sphere_pair(displacement, **params)\n",
    "\n",
    "    force_fn = quantity.force(energy_fn)\n",
    "    force_fn = jit(force_fn)\n",
    "\n",
    "    # we need to use a scan instead of a while loop in order to use jax.grad\n",
    "    solver = lambda f, x: run_minimization_scan(f, x, shift, num_steps=num_steps,dt_start=0.001, dt_max=0.005)[0]\n",
    "    R_final = solver(force_fn,R_init)\n",
    "    \n",
    "    return energy_fn(R_final), jnp.amax(jnp.abs(force_fn(R_final)))\n",
    "\n",
    "(exp_e, exp_f), exp_g = jax.value_and_grad(explicit_diff,has_aux=True)(param_dict,R_init,displacement,19400)\n",
    "print('Energy        : ',exp_e)\n",
    "print('Max_grad_force: ',exp_f)\n",
    "print('Gradient of the energy:')\n",
    "print(exp_g['sigma'][0])\n",
    "print(exp_g['alpha'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "veterinary-portrait",
   "metadata": {},
   "source": [
    "This being plain `jax` code we can easily compute gradients with respect to different parameters in one go."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "interpreted-madness",
   "metadata": {},
   "source": [
    "Now what's the problem here?\\\n",
    "The answer to that is quite simple and easily demonstrated by working with a larger system and/or a system where we need to take more optimization steps to compute the energy minimum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "earlier-eagle",
   "metadata": {},
   "outputs": [],
   "source": [
    "# increase the number of particles\n",
    "N = 55\n",
    "# and the density\n",
    "density = 0.5\n",
    "\n",
    "dimension = 2\n",
    "box_size = quantity.box_size_at_number_density(N,density,dimension)\n",
    "displacement, shift = space.periodic(box_size) \n",
    "\n",
    "key = random.PRNGKey(5)\n",
    "key, split = random.split(key)\n",
    "R_init = random.uniform(key, (N,dimension), minval=0.0, maxval=box_size, dtype=f64) \n",
    "\n",
    "sigma = jnp.full((N,N), 2.0)\n",
    "alpha = jnp.full((N,N),2.)\n",
    "param_dict = {'sigma':sigma,'alpha':alpha}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "communist-unknown",
   "metadata": {},
   "source": [
    "For this system we now have to take nearly 10 times more steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "crude-utilization",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do not run this cell locally! This cell realiably crashes my mac.\n",
    "(exp_e_large, exp_f_large), exp_g_large = jax.value_and_grad(explicit_diff,has_aux=True)(param_dict,R_init,displacement,163954)\n",
    "print('Energy        : ',exp_e_large)\n",
    "print('Max_grad_force: ',exp_f_large)\n",
    "print('Gradient of the energy:')\n",
    "print(exp_g_large['sigma'][0])\n",
    "print(exp_g_large['alpha'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "expressed-lightweight",
   "metadata": {},
   "source": [
    "As you can see our computation fails due to running out of memory when trying to compute the gradient. The reason for that is that for reverse mode differentiation (e.g. ```jax.grad```) the memory consumption grows linearly with respect to the number of optimization steps we have to take since reverse mode differentiation needs to store the whole forward pass in order to compute the gradient in the backwards pass."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mighty-institution",
   "metadata": {},
   "source": [
    "We could lessen the memory requirements by using a technique called [gradient rematerialization/checkpointing](https://github.com/google/jax/blob/f3c4ae3d8918de3a35cec74fdf24231a77ef0e92/jax/_src/api.py#L2806) for a corresponding increase in computation time. While this strategy works, there exists a better solution for our problem called implicit differentiation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abroad-charter",
   "metadata": {},
   "source": [
    "# Implicit Differentiation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nominated-bunny",
   "metadata": {},
   "source": [
    "Implicit differentiation gets its name from the [implicit function theorem](https://en.wikipedia.org/wiki/Implicit_function_theorem). This theorem roughly states that when we want to differentiate through a root finding procedure, e.g. find $z$ such that $F(a,z) = 0$, it does not matter how we computed the solution. Instead it is possible to directly differentiate through the solution $z^*$ of our root finding problem using the following formular. Here $\\partial_i$ means we differentiate with respect to the $i$'th argument."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "isolated-drove",
   "metadata": {},
   "source": [
    "$$\n",
    "\\partial z^*(a) = -[\\partial_1 f(a_0,z_0))]^{-1} \\partial_0 f(a_0,z_0))\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "agricultural-rebel",
   "metadata": {},
   "source": [
    "This expression can be efficiently solved. If you are interested in a derivation and how one could implement this in `jax` then I can highly recommend chapter $2$ of the NeurIPS 2020 [Deep Implicit Layers](https://implicit-layers-tutorial.org/) tutorial.\n",
    "\n",
    "In order to use implicit differentiation we first need to rewrite our optimization problem as a root finding procedure. This is easily done since the force is $0$ at the energy minimum.\n",
    "\n",
    "I'll first define a new function that uses implicit differentiation and then I'll highlight the differences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "consistent-singing",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's go back to our small system in order to compare explicit and implicit differentiation.\n",
    "N = 7\n",
    "density = 0.3\n",
    "\n",
    "dimension = 2\n",
    "box_size = quantity.box_size_at_number_density(N,density,dimension)\n",
    "displacement, shift = space.periodic(box_size) \n",
    "\n",
    "key = random.PRNGKey(5)\n",
    "key, split = random.split(key)\n",
    "R_init = random.uniform(key, (N,dimension), minval=0.0, maxval=box_size, dtype=f64) \n",
    "\n",
    "sigma = jnp.full((N,N), 2.0)\n",
    "alpha = jnp.full((N,N),2.)\n",
    "param_dict = {'sigma':sigma,'alpha':alpha}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mature-discharge",
   "metadata": {},
   "outputs": [],
   "source": [
    "def implicit_diff(params,R_init,displacement):\n",
    "    energy_fn = energy.soft_sphere_pair(displacement, **params)\n",
    "    force_fn = jit(quantity.force(energy_fn))\n",
    "    \n",
    "    # wrap force_fn with a lax.stop_gradient to prevent a CustomVJPException\n",
    "    no_grad_force_fn = jit(lambda x: lax.stop_gradient(force_fn(x)))\n",
    "    \n",
    "    # make the dependence on the variables we want to differentiate explicit\n",
    "    explicit_force_fn = jit(lambda R, p: force_fn(R, **p))\n",
    "    \n",
    "    def solver(params, x):\n",
    "        # params are unused\n",
    "        del params\n",
    "        # need to use no_grad_force_fn!\n",
    "        return run_minimization_while(no_grad_force_fn, x, shift, dt_start=0.001, dt_max=0.005)[0]\n",
    "    \n",
    "    decorated_solver = jaxopt.implicit_diff.custom_root(explicit_force_fn)(solver)\n",
    "    \n",
    "    R_final = decorated_solver(None,R_init)\n",
    "    \n",
    "    # Here we can just use our original energy_fn/force_fn\n",
    "    return energy_fn(R_final), jnp.amax(jnp.abs(force_fn(R_final)))\n",
    "\n",
    "(imp_e, imp_f), imp_g = jax.value_and_grad(implicit_diff,has_aux=True)(param_dict,R_init,displacement)\n",
    "print('Energy        : ',imp_e)\n",
    "print('Max_grad_force: ',imp_f)\n",
    "print('Gradient of the energy:')\n",
    "print(imp_g['sigma'][0])\n",
    "print(imp_g['alpha'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "biblical-pharmacy",
   "metadata": {},
   "source": [
    "As you can see we do not have to change anything compared to our explicit version when we want to compute gradients with respect to more then one parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dried-shame",
   "metadata": {},
   "outputs": [],
   "source": [
    "# As you can see the we get the same results.\n",
    "print(jax.tree_map(jnp.allclose,exp_e,imp_e))\n",
    "print(jax.tree_map(jnp.allclose,exp_f,imp_f))\n",
    "print(jax.tree_map(jnp.allclose,exp_g,imp_g))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "happy-aviation",
   "metadata": {},
   "source": [
    "We use [jaxopt](https://github.com/google/jaxopt) to define implicit gradients for our energy minimization routine. To this end we need to define two new force functions from our original force_fn. This is necessary for two seperate reasons. \n",
    "\n",
    "For one [jaxopt.implicit_diff.custom root](https://jaxopt.github.io/stable/_autosummary/jaxopt.implicit_diff.custom_root.html#jaxopt.implicit_diff.custom_root) requires a function that takes the parameter that we want to differentiate as an explicit input. To this end we define a new explicit_force_fn as\n",
    "```python \n",
    "explicit_force_fn = jit(lambda R, p: force_fn(R,**p))\n",
    "```\n",
    "\n",
    "Furthermore we cannot pass our original force_fn to our solver as this will cause a ```CustomVJPException```. We can just wrap the output of our ```force_fn``` with ```lax.stop_gradient``` in order to fix this issue.\n",
    "```python\n",
    "no_grad_force_fn = jit(lambda x: lax.stop_gradient(force_fn(x)))\n",
    "```\n",
    "\n",
    "Having done that we can define our ```solver``` which takes two parameters as its input. A dummy variable ```params```, which we can just delete as we do not need it and a variable ```x``` which is our set of initial positions. It is necessary that we pass our newly defined ```no_grad_force_fn``` to our solver.\n",
    "\n",
    "Now we can put it all together and define a ```decorated_solver``` using ```jaxopt``` in order to be able to efficiently differentiate through it:\n",
    "```python\n",
    "decorated_solver = implicit_diff.custom_root(explicit_force_fn)(solver)\n",
    "```\n",
    "Here we have to use our ```explicit_force_fn```. It is also possible to use a different linear solver for the ```vjp``` computation using the ```solve``` argument of `implicit_diff.custom_root`. Jax comes with a handfull of sparse linear solvers in ```jax.scipy.sparse.linalg```."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "removed-completion",
   "metadata": {},
   "source": [
    "# Adding neighbor lists and other auxiliary output to our solver"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wound-nirvana",
   "metadata": {},
   "source": [
    "Until now we have only worked with solvers that return one parameter `R_final`. Now we'll see how we have to change our `implicit_diff` function in order to be able to also have our solver return a neighbor list `nbrs` and the number of steps `num_steps` needed to reach the minimium.\n",
    "\n",
    "There is only one thing we have to change besides the normal stuff we have to change when using neighbor lists. We simply add the ```has_aux=True``` keyword to ```implicit_diff.custom_root```."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "massive-formation",
   "metadata": {},
   "source": [
    "There's one more funny thing to note about the neighbor list version. Instead of getting an ```CustomVJPException``` when we forget to wrap our ```force_fn``` with ```lax.stop_gradient``` we get no explicit error message but we now get a memory leak instead which drastically slows down the computation. So pay attention that your are passing the correct version of your ```force_fn``` to your ```solver```!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "consecutive-adapter",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We add the parameter forced_rebuilding to make sure \n",
    "# that it is possible to construct new neighbor lists during the optimization.\n",
    "\n",
    "def implicit_diff_nl(params,R_init,box_size,forced_rebuilding):\n",
    "    neighbor_fn, energy_fn = energy.soft_sphere_neighbor_list(displacement, box_size, **params)\n",
    "\n",
    "    force_fn = jit(quantity.force(energy_fn))\n",
    "    \n",
    "    # wrap force_fn with a lax.stop_gradient to prevent a memory leak\n",
    "    no_grad_force_fn = jit(lambda x,neighbor: lax.stop_gradient(force_fn(x,neighbor)))\n",
    "    \n",
    "    # make the dependence on the variables we want to differentiate explicit\n",
    "    explicit_force_fn = jit(lambda R, neighbor, p: force_fn(R,neighbor,**p))\n",
    "    \n",
    "    def solver(params, x):\n",
    "        # params are unused\n",
    "        del params\n",
    "        # need to use no_grad_force_fn!\n",
    "        return run_minimization_while_nl(neighbor_fn,no_grad_force_fn, x, forced_rebuilding, shift, dt_start=0.001, dt_max=0.005)\n",
    "    \n",
    "    # Need to use hax_aux=True\n",
    "    decorated_solver = jaxopt.implicit_diff.custom_root(explicit_force_fn,has_aux=True)(solver)\n",
    "    \n",
    "    R_final, nbrs, num_steps = decorated_solver(None,R_init)\n",
    "    \n",
    "    # Here we can just use our original energy_fn/force_fn\n",
    "    return energy_fn(R_final,neighbor=nbrs), jnp.amax(jnp.abs(force_fn(R_final,neighbor=nbrs)))\n",
    "\n",
    "(imp_nl_e, imp_nl_f), imp_nl_g = jax.value_and_grad(implicit_diff_nl,has_aux=True)(param_dict,R_init,box_size,True)\n",
    "print('Energy        : ',imp_nl_e)\n",
    "print('Max_grad_force: ',imp_nl_f)\n",
    "print('Gradient of the energy:')\n",
    "print(imp_nl_g['sigma'][0])\n",
    "print(imp_nl_g['alpha'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "behind-clark",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We again get the same results.\n",
    "print(jax.tree_map(jnp.allclose,exp_e,imp_e))\n",
    "print(jax.tree_map(jnp.allclose,exp_f,imp_f))\n",
    "print(jax.tree_map(jnp.allclose,exp_g,imp_g))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
